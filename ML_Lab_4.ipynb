{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Analytic Method**"
      ],
      "metadata": {
        "id": "mc7vlOYhX31b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Implement Linear Regression and calculate sum of residual error on the following Datasets.\n",
        "\n",
        " *x = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]*\n",
        "\n",
        " *y = [1, 3, 2, 5, 7, 8, 8, 9, 10, 12]*\n",
        "### Compute the regression coefficients using analytic formulation and calculate Sum Squared Error (SSE) and R2 value.\n",
        "### Implement gradient descent (both Full-batch and Stochastic with stopping criteria) on Least Mean Square loss formulation to compute the coefficients of regression matrix and compare the results using performance measures such as R2 SSE etc.\n",
        "\n"
      ],
      "metadata": {
        "id": "_eyExoU1cwcV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
        "Y = [1, 3, 2, 5, 7, 8, 8, 9, 10, 12]\n",
        "\n",
        "mean_X = sum(X) / len(X)\n",
        "mean_Y = sum(Y) / len(Y)\n",
        "\n",
        "num=0\n",
        "den = 0\n",
        "\n",
        "for i in range(len(X)):\n",
        "    num += (X[i] - mean_X) * (Y[i] - mean_Y)\n",
        "    den += (X[i] - mean_X) ** 2\n",
        "\n",
        "beta1 = num / den\n",
        "beta0 = mean_Y - beta1 * mean_X\n",
        "\n",
        "print(\"Slope (beta1):\", beta1)\n",
        "print(\"Intercept (beta0):\", beta0)\n",
        "\n",
        "#predicted Y and SSE\n",
        "predicted_Y = [beta1 * x + beta0 for x in X]\n",
        "SSE = sum((Y[i] - predicted_Y[i]) ** 2 for i in range(len(Y)))\n",
        "\n",
        "print(\"Sum of Squared Errors (SSE):\", SSE)\n",
        "\n",
        "#r2\n",
        "SS_total = sum((Y[i] - mean_Y) ** 2 for i in range(len(Y)))\n",
        "R_squared = 1 - (SSE / SS_total)\n",
        "\n",
        "print(\"R-square:\", R_squared)\n",
        "print(\"Linear Regression Equation is y = \",beta1,\"X + \", beta0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rnDzH7K3Oalj",
        "outputId": "fc36b4af-96ba-4465-a431-26629899d160"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Slope (beta1): 1.1696969696969697\n",
            "Intercept (beta0): 1.2363636363636363\n",
            "Sum of Squared Errors (SSE): 5.624242424242421\n",
            "R-square: 0.952538038613988\n",
            "Linear Regression Equation is y =  1.1696969696969697 X +  1.2363636363636363\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Full Batch Gradient Descent**"
      ],
      "metadata": {
        "id": "OLl3ymveXyIJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 0.01\n",
        "iterations = 1000\n",
        "beta0 = 0\n",
        "beta1 = 0\n",
        "\n",
        "# Full batch gradient descent\n",
        "for _ in range(iterations):\n",
        "    gradient_beta0 = 0\n",
        "    gradient_beta1 = 0\n",
        "    for i in range(len(X)):\n",
        "        gradient_beta0 += (beta0 + beta1 * X[i] - Y[i])\n",
        "        gradient_beta1 += (beta0 + beta1 * X[i] - Y[i]) * X[i]\n",
        "    beta0 = beta0 - (learning_rate * gradient_beta0 / len(X))\n",
        "    beta1 = beta1 - (learning_rate * gradient_beta1 / len(X))\n",
        "\n",
        "print(\"Full Batch Gradient Descent:\")\n",
        "print(\"Intercept (beta0):\", beta0)\n",
        "print(\"Slope (beta1):\", beta1)\n",
        "\n",
        "predicted_Y = [beta0 + beta1 * x for x in X]\n",
        "\n",
        "#SSE\n",
        "SSE = sum((Y[i] - predicted_Y[i]) ** 2 for i in range(len(Y)))\n",
        "\n",
        "print(\"Sum of Squared Errors (SSE):\", SSE)\n",
        "\n",
        "#r2\n",
        "mean_Y = sum(Y) / len(Y)\n",
        "SS_total = sum((Y[i] - mean_Y) ** 2 for i in range(len(Y)))\n",
        "R_squared = 1 - (SSE / SS_total)\n",
        "\n",
        "print(\"R-square:\", R_squared)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ub4kohArUrWy",
        "outputId": "f2cf6451-f6c2-4108-d29b-6afeb223930a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Full Batch Gradient Descent:\n",
            "Intercept (beta0): 1.175803611388339\n",
            "Slope (beta1): 1.1793547634798334\n",
            "Sum of Squared Errors (SSE): 5.634861529064237\n",
            "R-square: 0.9524484259150697\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Stochiastic Gradient Descent**"
      ],
      "metadata": {
        "id": "gJAK5qIAXoH5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 0.01\n",
        "iterations = 1000\n",
        "beta0 = 0\n",
        "beta1 = 0\n",
        "\n",
        "# Stochastic gradient descent\n",
        "for _ in range(iterations):\n",
        "    for i in range(len(X)):\n",
        "        gradient_beta0 = beta0 + beta1 * X[i] - Y[i]\n",
        "        gradient_beta1 = (beta0 + beta1 * X[i] - Y[i]) * X[i]\n",
        "        beta0 = beta0 - learning_rate * gradient_beta0\n",
        "        beta1 = beta1 - learning_rate * gradient_beta1\n",
        "\n",
        "print(\"\\nStochastic Gradient Descent:\")\n",
        "print(\"Intercept (beta0):\", beta0)\n",
        "print(\"Slope (beta1):\", beta1)\n",
        "\n",
        "predicted_Y = [beta0 + beta1 * x for x in X]\n",
        "\n",
        "#SSE\n",
        "SSE = sum((Y[i] - predicted_Y[i]) ** 2 for i in range(len(Y)))\n",
        "\n",
        "print(\"Sum of Squared Errors (SSE):\", SSE)\n",
        "\n",
        "#r2\n",
        "mean_Y = sum(Y) / len(Y)\n",
        "SS_total = sum((Y[i] - mean_Y) ** 2 for i in range(len(Y)))\n",
        "R_squared = 1 - (SSE / SS_total)\n",
        "\n",
        "print(\"R-square:\", R_squared)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P6s0zTGAWD2K",
        "outputId": "87e31add-8500-424f-a4aa-c5eb0a3a6bff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Stochastic Gradient Descent:\n",
            "Intercept (beta0): 1.1434234183192076\n",
            "Slope (beta1): 1.1924414227954474\n",
            "Sum of Squared Errors (SSE): 5.667805958642662\n",
            "R-square: 0.9521704138511168\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.Download Boston Housing Rate Dataset. Analyse the input attributes and find out the attribute that best follow the linear relationship with the output price. Implement both the analytic formulation and gradient descent (Full-batch, stochastic) on LMS lossformulation to compute the coefficients of regression matrix and compare the results."
      ],
      "metadata": {
        "id": "lxliEooOYCdT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import requests\n",
        "from io import StringIO\n",
        "\n",
        "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.data\"\n",
        "response = requests.get(url)\n",
        "data = response.text\n",
        "\n",
        "names = [\"CRIM\", \"ZN\", \"INDUS\", \"CHAS\", \"NOX\", \"RM\", \"AGE\", \"DIS\", \"RAD\", \"TAX\", \"PTRATIO\", \"B\", \"LSTAT\", \"MEDV\"]\n",
        "df = pd.read_csv(StringIO(data), delim_whitespace=True, names=names)\n",
        "\n",
        "X = df.drop(columns=['MEDV']).values\n",
        "y = df['MEDV'].values\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "correlation_coefficients = np.abs(np.corrcoef(X_train_scaled.T, y_train)[0, 1:])\n",
        "\n",
        "best_attribute_index = np.argmax(correlation_coefficients)\n",
        "best_attribute_name = names[best_attribute_index]\n",
        "print(\"Attribute with the highest correlation with price:\", best_attribute_name)\n",
        "\n",
        "X_train_with_bias = np.c_[np.ones((X_train_scaled.shape[0], 1)), X_train_scaled]\n",
        "analytic_solution = np.linalg.inv(X_train_with_bias.T.dot(X_train_with_bias)).dot(X_train_with_bias.T).dot(y_train)\n",
        "\n",
        "def full_batch_gradient_descent(X, y, learning_rate=0.01, n_iterations=1000):\n",
        "    m = len(y)\n",
        "    theta = np.random.randn(X.shape[1])\n",
        "    for iteration in range(n_iterations):\n",
        "        gradients = 2/m * X.T.dot(X.dot(theta) - y)\n",
        "        theta -= learning_rate * gradients\n",
        "    return theta\n",
        "\n",
        "def stochastic_gradient_descent(X, y, learning_rate=0.01, n_iterations=1000):\n",
        "    m = len(y)\n",
        "    theta = np.random.randn(X.shape[1])\n",
        "    for iteration in range(n_iterations):\n",
        "        for i in range(m):\n",
        "            random_index = np.random.randint(m)\n",
        "            xi = X[random_index:random_index+1]\n",
        "            yi = y[random_index:random_index+1]\n",
        "            gradients = 2 * xi.T.dot(xi.dot(theta) - yi)\n",
        "            theta -= learning_rate * gradients\n",
        "    return theta\n",
        "\n",
        "theta_full_batch = full_batch_gradient_descent(X_train_with_bias, y_train)\n",
        "\n",
        "theta_stochastic = stochastic_gradient_descent(X_train_with_bias, y_train)\n",
        "\n",
        "print(\"Analytic solution coefficients:\", analytic_solution)"
      ],
      "metadata": {
        "id": "kFWSc_vnX9lv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd807fac-2a5e-4cc2-fe39-dbbe6d28b16f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attribute with the highest correlation with price: DIS\n",
            "Analytic solution coefficients: [22.79653465 -1.00213533  0.69626862  0.27806485  0.7187384  -2.0223194\n",
            "  3.14523956 -0.17604788 -3.0819076   2.25140666 -1.76701378 -2.03775151\n",
            "  1.12956831 -3.61165842]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Full-batch gradient descent coefficients:\", theta_full_batch)"
      ],
      "metadata": {
        "id": "cvZGohlNdQLv",
        "outputId": "ab629658-18a2-4711-c046-9baffb7e5b29",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Full-batch gradient descent coefficients: [22.79653462 -0.95188403  0.56191963  0.10717462  0.74568816 -1.95105073\n",
            "  3.20563282 -0.17932266 -2.96328203  1.69982491 -1.15978267 -2.02592308\n",
            "  1.1305053  -3.58399282]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Stochastic gradient descent coefficients:\", theta_stochastic)"
      ],
      "metadata": {
        "id": "upuv-53tdR_W",
        "outputId": "81b5df9d-8315-4534-929e-121fd370a391",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stochastic gradient descent coefficients: [22.65500657 -0.42834202  1.34458471 -0.25347829 -0.63088908 -1.81602241\n",
            "  2.75898688 -0.16921792 -2.29956408  1.65546777 -2.50559512 -2.31705352\n",
            "  0.88137425 -2.82715737]\n"
          ]
        }
      ]
    }
  ]
}